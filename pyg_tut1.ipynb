{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "https://colab.research.google.com/drive/1DIQm9rOx2mT1bZETEeVUThxcrP1RKqAn\n",
    "\n",
    "http://web.stanford.edu/class/cs224w/index.html\n",
    "\n",
    "Graph Representation Learning Book:\n",
    "https://www.cs.mcgill.ca/~wlh/grl_book/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "## transformations\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "## download and load training dataset\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "## download and load testing dataset\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1,2],[3,4]])\n",
    "b = np.ones((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.nn as pyg_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.dropbox.com/s/1bnz8r7mofx0osf/net_aminer.zip?dl=1\n",
      "Extracting C:\\Users\\hmaan\\data\\AMiner\\net_aminer.zip\n",
      "Downloading https://www.dropbox.com/s/nkocx16rpl4ydde/label.zip?dl=1\n",
      "Extracting C:\\Users\\hmaan\\data\\AMiner\\raw\\label.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Step: 00100/13231, Loss: 9.0997\n",
      "Epoch: 1, Step: 00200/13231, Loss: 7.5686\n",
      "Epoch: 1, Step: 00300/13231, Loss: 6.4582\n",
      "Epoch: 1, Step: 00400/13231, Loss: 5.8532\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hmaan\\Documents\\GNNs\\pyg_tut1.ipynb Cell 6'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hmaan/Documents/GNNs/pyg_tut1.ipynb#ch0000007?line=76'>77</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39mtest(z[train_perm], y[train_perm], z[test_perm], y[test_perm],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hmaan/Documents/GNNs/pyg_tut1.ipynb#ch0000007?line=77'>78</a>\u001b[0m                       max_iter\u001b[39m=\u001b[39m\u001b[39m150\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hmaan/Documents/GNNs/pyg_tut1.ipynb#ch0000007?line=80'>81</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m6\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hmaan/Documents/GNNs/pyg_tut1.ipynb#ch0000007?line=81'>82</a>\u001b[0m     train(epoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hmaan/Documents/GNNs/pyg_tut1.ipynb#ch0000007?line=82'>83</a>\u001b[0m     acc \u001b[39m=\u001b[39m test()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hmaan/Documents/GNNs/pyg_tut1.ipynb#ch0000007?line=83'>84</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, Accuracy: \u001b[39m\u001b[39m{\u001b[39;00macc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\hmaan\\Documents\\GNNs\\pyg_tut1.ipynb Cell 6'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch, log_steps, eval_steps)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hmaan/Documents/GNNs/pyg_tut1.ipynb#ch0000007?line=49'>50</a>\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mloss(pos_rw\u001b[39m.\u001b[39mto(device), neg_rw\u001b[39m.\u001b[39mto(device))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hmaan/Documents/GNNs/pyg_tut1.ipynb#ch0000007?line=50'>51</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hmaan/Documents/GNNs/pyg_tut1.ipynb#ch0000007?line=51'>52</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hmaan/Documents/GNNs/pyg_tut1.ipynb#ch0000007?line=53'>54</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hmaan/Documents/GNNs/pyg_tut1.ipynb#ch0000007?line=54'>55</a>\u001b[0m \u001b[39mif\u001b[39;00m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m log_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/ai/lib/site-packages/torch/optim/optimizer.py?line=85'>86</a>\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/ai/lib/site-packages/torch/optim/optimizer.py?line=86'>87</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m---> <a href='file:///~/anaconda3/envs/ai/lib/site-packages/torch/optim/optimizer.py?line=87'>88</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\autograd\\grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/ai/lib/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/ai/lib/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/ai/lib/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[1;32m---> <a href='file:///~/anaconda3/envs/ai/lib/site-packages/torch/autograd/grad_mode.py?line=27'>28</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\optim\\sparse_adam.py:101\u001b[0m, in \u001b[0;36mSparseAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/ai/lib/site-packages/torch/optim/sparse_adam.py?line=97'>98</a>\u001b[0m             \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/ai/lib/site-packages/torch/optim/sparse_adam.py?line=98'>99</a>\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> <a href='file:///~/anaconda3/envs/ai/lib/site-packages/torch/optim/sparse_adam.py?line=100'>101</a>\u001b[0m     F\u001b[39m.\u001b[39;49msparse_adam(params_with_grad,\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ai/lib/site-packages/torch/optim/sparse_adam.py?line=101'>102</a>\u001b[0m                   grads,\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ai/lib/site-packages/torch/optim/sparse_adam.py?line=102'>103</a>\u001b[0m                   exp_avgs,\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ai/lib/site-packages/torch/optim/sparse_adam.py?line=103'>104</a>\u001b[0m                   exp_avg_sqs,\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ai/lib/site-packages/torch/optim/sparse_adam.py?line=104'>105</a>\u001b[0m                   state_steps,\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ai/lib/site-packages/torch/optim/sparse_adam.py?line=105'>106</a>\u001b[0m                   beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ai/lib/site-packages/torch/optim/sparse_adam.py?line=106'>107</a>\u001b[0m                   beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ai/lib/site-packages/torch/optim/sparse_adam.py?line=107'>108</a>\u001b[0m                   lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ai/lib/site-packages/torch/optim/sparse_adam.py?line=108'>109</a>\u001b[0m                   eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ai/lib/site-packages/torch/optim/sparse_adam.py?line=110'>111</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\optim\\_functional.py:497\u001b[0m, in \u001b[0;36msparse_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, state_steps, eps, beta1, beta2, lr)\u001b[0m\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ai/lib/site-packages/torch/optim/_functional.py?line=494'>495</a>\u001b[0m old_exp_avg_sq_values \u001b[39m=\u001b[39m exp_avg_sq\u001b[39m.\u001b[39msparse_mask(grad)\u001b[39m.\u001b[39m_values()\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ai/lib/site-packages/torch/optim/_functional.py?line=495'>496</a>\u001b[0m exp_avg_sq_update_values \u001b[39m=\u001b[39m grad_values\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msub_(old_exp_avg_sq_values)\u001b[39m.\u001b[39mmul_(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m--> <a href='file:///~/anaconda3/envs/ai/lib/site-packages/torch/optim/_functional.py?line=496'>497</a>\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49madd_(make_sparse(exp_avg_sq_update_values))\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ai/lib/site-packages/torch/optim/_functional.py?line=498'>499</a>\u001b[0m \u001b[39m# Dense addition again is intended, avoiding another sparse_mask\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ai/lib/site-packages/torch/optim/_functional.py?line=499'>500</a>\u001b[0m numer \u001b[39m=\u001b[39m exp_avg_update_values\u001b[39m.\u001b[39madd_(old_exp_avg_values)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Reaches around 91.8% Micro-F1 after 5 epochs.\n",
    "\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch_geometric.datasets import AMiner\n",
    "from torch_geometric.nn import MetaPath2Vec\n",
    "\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import AGNNConv\n",
    "\n",
    "#dataset = 'Cora'\n",
    "#path = osp.join(osp.dirname(osp.realpath(\"f1\")), '..', 'data', dataset)\n",
    "#dataset = Planetoid(path, dataset, transform=T.NormalizeFeatures())\n",
    "#data = dataset[0]\n",
    "\n",
    "path = osp.join(osp.dirname(osp.realpath(\"f1\")), '../../data/AMiner')\n",
    "dataset = AMiner(path)\n",
    "data = dataset[0]\n",
    "\n",
    "metapath = [\n",
    "    ('author', 'writes', 'paper'),\n",
    "    ('paper', 'published_in', 'venue'),\n",
    "    ('venue', 'publishes', 'paper'),\n",
    "    ('paper', 'written_by', 'author')\n",
    "]\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = MetaPath2Vec(data.edge_index_dict, embedding_dim=128,\n",
    "                     metapath=metapath, walk_length=50, context_size=7,\n",
    "                     walks_per_node=5, num_negative_samples=5,\n",
    "                     sparse=True).to(device)\n",
    "\n",
    "loader = model.loader(batch_size=128, shuffle=True, num_workers=6)\n",
    "optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
    "\n",
    "\n",
    "def train(epoch, log_steps=100, eval_steps=2000):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for i, (pos_rw, neg_rw) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if (i + 1) % log_steps == 0:\n",
    "            print((f'Epoch: {epoch}, Step: {i + 1:05d}/{len(loader)}, '\n",
    "                   f'Loss: {total_loss / log_steps:.4f}'))\n",
    "            total_loss = 0\n",
    "\n",
    "        if (i + 1) % eval_steps == 0:\n",
    "            acc = test()\n",
    "            print((f'Epoch: {epoch}, Step: {i + 1:05d}/{len(loader)}, '\n",
    "                   f'Acc: {acc:.4f}'))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(train_ratio=0.1):\n",
    "    model.eval()\n",
    "\n",
    "    z = model('author', batch=data['author'].y_index)\n",
    "    y = data['author'].y\n",
    "\n",
    "    perm = torch.randperm(z.size(0))\n",
    "    train_perm = perm[:int(z.size(0) * train_ratio)]\n",
    "    test_perm = perm[int(z.size(0) * train_ratio):]\n",
    "\n",
    "    return model.test(z[train_perm], y[train_perm], z[test_perm], y[test_perm],\n",
    "                      max_iter=150)\n",
    "\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train(epoch)\n",
    "    acc = test()\n",
    "    print(f'Epoch: {epoch}, Accuracy: {acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c314c87e776466ef4f077dc585c9dc3eaee46cc84096b0d5b81aa8c7c1fafbb2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
